---
id: 219
title: Replication, psychology, and big science
date: 2012-04-18T15:29:00+00:00
author: admin
tumblr_simplystatistics_permalink:
  - http://simplystatistics.tumblr.com/post/21326470429/replication-psychology-and-big-science
tumblr_simplystatistics_id:
  - 21326470429
dsq_thread_id:
  - 934401893
categories:
  - Uncategorized
tags:
  - replicability
  - reproducibility
slug: "replication-psychology-and-big-science"
---
<a href="http://www.sciencemag.org/content/334/6060/1226.full" target="_blank">Reproducibility</a> <a href="http://simplystatistics.tumblr.com/post/12328728291/interview-with-victoria-stodden" target="_blank">has been</a> a <a href="http://simplystatistics.tumblr.com/post/13780369155/preventing-errors-through-reproducibility" target="_blank">hot topic</a> for the last several years among computational scientists. A study is reproducible if there is a specific set of computational functions/analyses (usually specified in terms of code) that exactly reproduce all of the numbers in a published paper from raw data. It is now recognized that a critical component of the scientific process is that data analyses can be reproduced. This point has been driven home particularly for personalized medicine applications, where irreproducible results <a href="http://www.nature.com/news/lapses-in-oversight-compromise-omics-results-1.10298?nc=1332884191164" target="_blank">can lead to delays</a> in evaluating new procedures that affect patients&#8217; health. 

But just because a study is reproducible does not mean that it is _replicable_. Replicability is stronger than reproducibility. A study is only replicable if you perform the exact same experiment (at least) twice, collect data in the same way both times, perform the same data analysis, and arrive at the same conclusions. The difference with reproducibility is that to achieve replicability, you have to perform the experiment and collect the data again. This of course introduces all sorts of new potential sources of error in your experiment (new scientists, new materials, new lab, new thinking, different settings on the machines, etc.)

Replicability is getting a lot of attention recently in psychology due to some high-profile studies that did not replicate. First, there was the highly-cited experiment that<a href="http://blogs.discovermagazine.com/notrocketscience/2012/03/10/failed-replication-bargh-psychology-study-doyen/" target="_blank"> failed to replicate</a>, leading to a show down between the author of the original experiment and the replicators. Now there is a psychology project that allows researchers to post the results of <a href="http://www.sciencemag.org/content/335/6076/1558" target="_blank">replications of experiments</a> - whether they succeeded or failed. Finally, the <a href="http://openscienceframework.org/project/shvrbV8uSkHewsfD4/wiki/index" target="_blank">Reproducibility Project</a>, probably better termed the Replicability Project, seeks to <a href="http://chronicle.com/blogs/percolator/is-psychology-about-to-come-undone/29045?sid=at&utm_source=at&utm_medium=en" target="_blank">replicate the results</a> of every experiment in the journals _Psychological Science, _the_ Journal of Personality and Social Psychology,_or the_ Journal of Experimental Psychology: Learning, Memory, and Cognition _in the year 2008.

Replicability raises important issues for &#8220;big science&#8221; projects, ranging from genomics (<a href="http://www.1000genomes.org/" target="_blank">The Thousand Genomes Project</a>) to physics (<a href="http://en.wikipedia.org/wiki/Large_Hadron_Collider" target="_blank">The Large Hadron Collider</a>). These experiments are too big and costly to actually replicate. So how do we know the results of these experiments aren&#8217;t just errors, that upon replication (if we could do it) would not show up again? Maybe smaller scale replications of sub-projects could be used to help convince us of discoveries in these big projects?

In the meantime, I love the idea that replication is getting the credit it deserves (at least in psychology). The incentives in science often only credit the first person to an idea, not the long tail of folks who replicate the results. For example, replications of experiments are often not considered interesting enough to publish. Maybe these new projects will start to change some of the <a href="http://blog.regehr.org/archives/632" target="_blank">perverse academic incentives</a>.
